<!doctype html>
<html lang="en">


<!-- === Header Starts === -->
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Look&Listen: Multi-Modal Correlation Learning for Active Speaker Detection and Speech Enhancement</title>

  <link href="./assets/bootstrap.min.css" rel="stylesheet">
  <link href="./assets/font.css" rel="stylesheet" type="text/css">
  <link href="./assets/style.css" rel="stylesheet" type="text/css">
</head>
<!-- === Header Ends === -->


<body>


<!-- === Home Section Starts === -->
<div class="section">
  <!-- === Title Starts === -->
  <div class="header">
    <div class="title", style="padding-top: 25pt;">  <!-- Set padding as 10 if title is with two lines. -->
      Look&Listen: Multi-Modal Correlation Learning for Active Speaker Detection and Speech Enhancement
    </div>
  </div>
  <!-- === Title Ends === -->
  <div class="author">
      <a href="#" target="_blank"> Junwen Xiong<sup>1*</sup>,&nbsp;</a>
      <a href="#" target="_blank"> Yu Zhou<sup>1*</sup>,&nbsp;</a>
      <a href="#" target="_blank"> Peng Zhang<sup>1*†</sup>,</a>
      <a href="https://scholar.google.com.hk/citations?user=Qddov9wAAAAJ" target="_blank">Lei Xie<sup>1</sup>,</a>
      
        <a href="#" target="_blank">Wei Huang<sup>2</sup>,</a>
        <a href="#" target="_blank"> Yufei Zha<sup>1</sup>,</a> 
    </div>
  <div class="institution">
    School of Computer Science, ASGO, Northwestern Polytechnical University, Xi’an, China<sup>1</sup>
    </div>
    <div class="institution">
     School of Mathematics and Computer Sciences, Nanchang University, China<sup>2</sup>
  </div>
  <!-- <div class="conference">
    
  </div> -->
  <div class="link" style="font-size: 14pt;">
    <a href="https://arxiv.org/abs/2203.02216" target="_blank">[Paper]</a>&nbsp;
    <a href="https://github.com/Overcautious/ADENet" target="_blank">[Code]</a>
  </div>
  <!-- <div class="title">
    <img src="./assets/teasar.png" width="550" align="middle">
  </div> -->
</div>
<!-- === Home Section Ends === -->


<!-- === Overview Section Starts === -->
<div class="section">
  <div class="title">Overview</div>
  <div class="body">
    We propose a unified learning framework to jointly achieve active speaker detection and audio-visual speech enhancement. Firstly, we introduce a cross-modal conformer which is used to anticipate and model the temporal audio-visual relationships across spatial-temporal space. Then,  a plug-and-play multi-modal layer normalization is designed to alleviate the distribution misalignment of multi-modal features. Lastly, a cross-modal circulant fusion scheme is proposed to enable intrinsic assistance of two tasks by audio-visual feature interaction. In comparison to other state-of-the-art works, the proposed work shows a  superior performance for active speaker detection and audio visual speech enhancement on three benchmark datasets.
  </div>
</div>
<!-- === Overview Section Ends === -->


<!-- === Overview Section Starts === -->
<div class="section">
  <div class="title">Method</div>
  <!-- <div class="title">
    <img src="./assets/Figure1.png" width="800">
  </div> -->


  <div class="title">
    <img src="./assets/Figure3.png" width="800">
  </div>
  <div class="body">
    
    We propose a unified learning framework to jointly learn active speaker detection and audio-visual speech enhancement. To further achieve the mutual learning of both audio enhancement and visual detection, a scheme of cross-modal circulant fusion is proposed to leverage the complementary cues between the bifurcated processes for the establishment of their associations. By detecting active speakers with the help of enhanced audio information, the more accurate the detection result, the more reliable visual information to guide the speech enhancement; the cleaner the enhanced sound, the more distinctive the audio embedding, which will in turn help the detection, and overall performance can be guaranteed in such cyclic mutual learning.
  </div>
  <br></br>
  <div class="title">
    <img src="./assets/Figure1.png" width="800">
  </div>
  <div class = "body">
  <font size="4" color="blue">The overall pipeline of our proposed ADENet model</font>. Its framework is divided into three stages: audio-visual correlation learning, audio contextual learning, and cross-modal circulant fusion. The audio-visual correlation learning and the audio contextual learning aim at modeling the associations between multi-modal data and extracting contextual embeddings in the audio domain, respectively. Then, cross-modal circulant fusion is proposed to integrate correlation features and contextual features for active speaker detection and speech enhancement.
  </div>
  <br>
  
  </br>
  <div class="title">
    <img src="./assets/Figure4.png" width="650">
  </div>
  <font size="4" color="blue">Visualization of the feature distribution alignment</font>. (a) shows the distribution and topology of <font size="4" color="blue">audio</font> and <font size="4" color="green">visual</font> features. It is obviously that visual features are not aligned with audio ones. Thanks to the multi-modal layer normalization, visual and auditory features are brought into similar distributions as (b).
    <!-- Visualization of the multimodal prediction results on Argoverse validation set. We utilize all trajectory proposals to generate multiple trajectories for each scenario and visualize all the predicted endpoints (black background) in the figures. Colored points indicate the prediction results of a specific group of proposals (after filtering by score). We observe that the endpoints generated by each group of regional proposals are within the associated region. -->
</div>
<!-- === Overview Section Ends === -->


<!-- === Result Section Starts === -->
<div class="section">
  <div class="title">Results</div>
  <div class="body">
    Qualitative results of audio-visual sound enhancement for both audible and silent objects:
    <!-- Adjust the number of rows and columns (EVERY project differs). -->
    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./assets/Figure2.png" width="90%"></td>
      </tr>
    </table>
    Demo video link: 
    <br>
<!--     (Note: For each agent, <font size="4" color="green">best prediction result</font> is visualized) -->
    <!-- Adjust the frame size based on the demo (EVERY project differs). -->
    <div style="position: relative; padding-top: 50%; margin: 20pt 0; text-align: center;">
      <iframe src="https://www.youtube.com//embed/KKTXldx11pk" 
              title="Demo video of mmTransformer."
              frameborder=0
              style="position: absolute; top: 2.5%; left: 2.5%; width: 95%; height: 100%;"
              allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
              allowfullscreen></iframe>
    </div>
    <div> 
      Demo video of active speaker detection and speech enhancement result by ADENet.
    </div>
    <br>
    Also can be found in:
    <a href="#" target="_blank">[bilibili]</a>
  </div>
</div>
<!-- === Result Section Ends === -->


<!-- === Reference Section Starts === -->
<div class="section">
  <div class="bibtex">BibTeX</div>
<pre>
  @article{xiong2022look,
    title={Look$\backslash$\&Listen: Multi-Modal Correlation Learning for Active Speaker Detection and Speech Enhancement},
    author={Xiong, Junwen and Zhou, Yu and Zhang, Peng and Xie, Lei and Huang, Wei and Zha, Yufei},
    journal={arXiv preprint arXiv:2203.02216},
    year={2022}
  }
</pre>

  <!-- BZ: we should give other related work enough credits, -->
  <!--     so please include some most relevant work and leave some comment to summarize work and the difference. -->
  <!-- <div class="ref">Related Work</div>
  <div class="citation">
    <div class="image"><img src="./assets/tnt.png"></div>
    <div class="comment">
      <a href="https://arxiv.org/pdf/2008.08294.pdf" target="_blank">
        Hang Zhao, Jiyang Gao, Tian Lan, Chen Sun, Benjamin Sapp, Balakrishnan Varadarajan, Yue Shen, Yi Shen, Yuning Chai, Cordelia Schmid, Congcong Li, Dragomir Anguelov.
        TNT: Target-driveN Trajectory Prediction.
        CoRL 2020.</a><br> -->
      <!-- <b>Comment:</b>
      This is a short comment. -->
    <!-- </div>
  </div>
  <div class="citation">
    <div class="image"><img src="./assets/vec2.png"></div>
    <div class="comment">
      <a href="https://arxiv.org/pdf/2005.04259.pdf" target="_blank">
        Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir Anguelov, Congcong Li, Cordelia Schmid.
        VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation.
        CVPR 2020.</a><br> -->
      <!-- <b>Comment:</b>
      This is a long comment. This comment is just used to test how long comments can fit the template. -->
    <!-- </div>
  </div>
  <div class="citation">
    <div class="image"><img src="./assets/TP.png"></div>
    <div class="comment">
      <a href="https://arxiv.org/pdf/2004.12255.pdf" target="_blank">
        Liangji Fang, Qinhong Jiang, Jianping Shi, Bolei Zhou.
        TPNet: Trajectory Proposal Network for Motion Prediction.
        CVPR 2020.</a><br> -->
      <!-- <b>Comment:</b>
      This is a long comment. This comment is just used to test how long comments can fit the template. -->
    <!-- </div>
  </div>
</div> -->
<!-- === Reference Section Ends === -->


</body>
</html>
